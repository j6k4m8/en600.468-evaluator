#!/usr/bin/env python
import argparse
from itertools import islice # slicing for iterators
import requests

try:
    from nltk.corpus import wordnet as wn
    LOCAL_WORDNET = True
except:
    LOCAL_WORDNET = False

ALTERVISTA_KEY = '4UJdDj8dOAEoUDzf3uMj'
SYNONYMS = {}


def is_ascii(s):
    return all(ord(c) < 128 for c in s)


def get_synonyms(word):

    if not is_ascii(word):
        return set()

    if word in SYNONYMS:
        return SYNONYMS[word]

    if LOCAL_WORDNET:
        synonyms = set()
        for syn in wn.synsets(word):
            synonyms.update(syn.lemma_names())
        SYNONYMS[word] = synonyms
        return synonyms
    else:
        resp = requests.get('http://thesaurus.altervista.org/'
                            'thesaurus/v1?word={}&language=en_US'
                            '&key={}&output=json'
                            .format(w, ALTERVISTA_KEY)).json()
        if 'error' in resp:
            return []
        synonyms = set([
            syn.split(' ')[0] for syn in (
                '|'.join(
                    [resp['response'][i]['list']['synonyms']
                    for i in range(len(resp['response']))])
                .split('|')
            )
        ])
        SYNONYMS[word] = synonyms
        return synonyms

class Evaluators:

    @staticmethod
    def DEFAULT(hyp, ref):
        return 1

    @staticmethod
    def WORD_MATCHES(h, ref):
        """
        Outrageously slow.
        """
        return sum(1 for w in h if w in ref)

    @staticmethod
    def WORDNET(h, ref, weight=0.15):
        """
        Identical to WORD_MATCHES, but synonym matches are weighted according
        to `weight`. ("ball|ball" = 1, "ball|sphere" = weight)
        """
        running = 0
        for w in h:
            if w in ref:
                running += 1
            else:
                syns = get_synonyms(w)
                for s in syns:
                    if s in ref:
                        running += weight
                        break
        return running

    @staticmethod
    def METEOR(h, ref, alpha=0.9):
        """
        Implements the METEOR algorithm as described in the HW prompt
        <http://mt-class.org/jhu/hw3.html>.
        """
        h = set(h)
        e = set(ref)

        R = float(len(h.intersection(e))) / len(e)
        P = float(len(h.intersection(e))) / len(h)

        if P == R == 0:
            # Handle the corner-case of total non-correlation
            return 0

        return (P * R) / (((1-alpha) * R) + (alpha * (P)))

def evaluate(h1, h2, ref, eval_fns=Evaluators.DEFAULT):
    """
    Evaulates two hypotheses against a reference, using evaluator function
    as specified in eval_fns. This is a simple wrapper for the evaluator to
    prevent making changes to main().

    Arguments:
        h1 (str): The first hypothesis
        h2 (str): The second hypothesis
        ref (str): The reference sentence
        eval_fns (enum): Which function to use to evaluate,
            or a list of tuples to reach quorum

    Returns:
        int in (-1, 0, 1), 1 if 1 is better, -1, if 2 is better
    """
    if type(eval_fns) is not list:
        eval_fns = [(eval_fns, 1.)]

    res1 = sum([ef(h1, ref) * w for ef, w in eval_fns])
    res2 = sum([ef(h2, ref) * w for ef, w in eval_fns])

    if res1 > res2:
        return 1
    elif res2 > res1:
        return -1
    else:
        return 0


def main():
    parser = argparse.ArgumentParser(description='Evaluate hypotheses.')
    parser.add_argument('-i', '--input', default='data/hyp1-hyp2-ref',
                        help='input file (default data/hyp1-hyp2-ref)')
    parser.add_argument('-n', '--num_sentences', default=None, type=int,
                        help='Number of hypothesis pairs to evaluate')
    opts = parser.parse_args()

    # we create a generator and avoid loading all sentences into a list
    def sentences():
        with open(opts.input) as f:
            for pair in f:
                yield [
                    sentence.strip().split()
                    for sentence in pair.split(' ||| ')]

    for h1, h2, ref in islice(sentences(), opts.num_sentences):
        # We have two hypotheses and a reference.
        # We excise the logic to evaluate these hypotheses to an external
        # function so that we can easily make modifications to the protocol
        # without having to rewrite main().
        print evaluate(h1, h2, ref, [
            (Evaluators.METEOR, 0.85),
            (Evaluators.WORDNET, 0.25),
        ])


# convention to allow import of this file as a module
if __name__ == '__main__':
    main()
